{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install PyAudioWPatch\n",
    "\n",
    "from scipy.signal import resample\n",
    "import pyaudiowpatch as pyaudio\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torch\n",
    "import wave\n",
    "\n",
    "def save_wave(audio_frames, idx):\n",
    "    # Save the recorded audio to a WAV file\n",
    "    wav_file = f\"temp_audio_{idx}.wav\"\n",
    "    with wave.open(wav_file, 'wb') as wf:\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "        wf.setframerate(WISPER_RATE)\n",
    "        wf.writeframes(b''.join(audio_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loopback device is {'index': 28, 'structVersion': 2, 'name': 'Headset (realme Buds Wireless 3 Hands-Free AG Audio) [Loopback]', 'hostApi': 2, 'maxInputChannels': 1, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.003, 'defaultLowOutputLatency': 0.0, 'defaultHighInputLatency': 0.01, 'defaultHighOutputLatency': 0.0, 'defaultSampleRate': 16000.0, 'isLoopbackDevice': True}\n",
      "Listening for audio... Speak now.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "MIXED_PRECISION = True\n",
    "INPUT_DEVICE = {1:'pc_micro_phone', 2:'pc_speaker', 3:'bluetooth_speaker', 4:'bluetooth_microphone'}\n",
    "\"\"\" for pc_speaker, go to the sound icon and right click-> choose sounds -> recordings -> choose streo Mix as default  \"\"\"\n",
    "INPUT_DEVICE_IDX = 3\n",
    "\n",
    "\n",
    "device='cuda'\n",
    "# Optimize PyTorch for CPU\n",
    "torch.set_num_threads(6)  # Adjust to the number of CPU cores available\n",
    "# Load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to(device)\n",
    "# forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"transcribe\")#for french to french\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"translate\")#for french to english\n",
    "# forced_decoder_ids = None # for english to english\n",
    "\n",
    "\n",
    "\n",
    "# Initialize PyAudio\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "#  TODO create a nice function from this\n",
    "if INPUT_DEVICE[INPUT_DEVICE_IDX] == 'pc_speaker':\n",
    "    default_speakers = p.get_default_input_device_info()\n",
    "\n",
    "elif INPUT_DEVICE[INPUT_DEVICE_IDX] == 'bluetooth_speaker':\n",
    "    # Get default WASAPI info\n",
    "    wasapi_info = p.get_host_api_info_by_type(pyaudio.paWASAPI)\n",
    "    # Get default WASAPI speakers\n",
    "    default_speakers = p.get_device_info_by_index(wasapi_info[\"defaultOutputDevice\"])\n",
    "    if not default_speakers[\"isLoopbackDevice\"]:\n",
    "        for loopback in p.get_loopback_device_info_generator():\n",
    "            \"\"\"\n",
    "            Try to find loopback device with same name(and [Loopback suffix]).\n",
    "            Unfortunately, this is the most adequate way at the moment.\n",
    "            \"\"\"\n",
    "            if default_speakers[\"name\"] in loopback[\"name\"]:\n",
    "                default_speakers = loopback\n",
    "                break\n",
    "else:\n",
    "    pass\n",
    "\n",
    "print(f'The loopback device is {default_speakers}')\n",
    "\n",
    "# Settings for recording audio\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = int(default_speakers['defaultSampleRate'] ) \n",
    "WISPER_RATE = 16000# Whisper expects 16kHz input\n",
    "CHUNK = 1024  # Number of frames per buffer\n",
    "TRANSCRIPTION_INTERVAL = 30  # Interval for transcription in seconds\n",
    "\n",
    "# Open a stream to record audio\n",
    "\n",
    "stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK, input_device_index=default_speakers[\"index\"])\n",
    "\n",
    "print(\"Listening for audio... Speak now.\")\n",
    "\n",
    "audio_buffer = np.array([], dtype=np.float32)  # Buffer to store accumulated audio\n",
    "last_transcription_time = time.time()  # Initialize the last transcription time\n",
    "audio_frames = []  # Store raw audio frames\n",
    "\n",
    "# Open a text file to save the transcriptions\n",
    "transcription_file = open(\"transcriptions_cpu.txt\", \"a\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "\n",
    "        # Read a chunk of audio\n",
    "        data = stream.read(CHUNK, exception_on_overflow=False)  # Read a chunk of audio (1024 samples per chunk\n",
    "        audio_frames.append(data)  # Save raw audio data for MP3 conversion\n",
    "        audio_chunk = np.frombuffer(data, np.int16).flatten().astype(np.float32) / 32768.0\n",
    "        downsampled_chunk = resample(audio_chunk, int(len(audio_chunk) * WISPER_RATE / RATE))  # Resample to 16kHz\n",
    "\n",
    "        print(np.abs(audio_chunk).mean()> 0.02)\n",
    "\n",
    "        # # Only append audio with sound (ignoring silence)\n",
    "        # if np.abs(audio_chunk).mean() > 0.01:\n",
    "        #     audio_buffer = np.append(audio_buffer, downsampled_chunk)\n",
    "\n",
    "        # # Check if it's time to perform transcription\n",
    "        # current_time = time.time()\n",
    "        # if current_time - last_transcription_time >= TRANSCRIPTION_INTERVAL:\n",
    "        #     if audio_buffer.size > 0:  # Ensure there's audio to transcribe\n",
    "\n",
    "        #         start_translation_time = time.time()\n",
    "\n",
    "        #         input_features = processor(audio_buffer, sampling_rate=WISPER_RATE, return_tensors=\"pt\").input_features\n",
    "        #         # Generate token ids\n",
    "        #         predicted_ids = model.generate(input_features.to(device), forced_decoder_ids=forced_decoder_ids)\n",
    "        #         # Decode token ids to text\n",
    "        #         transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "        #         transcription_text = transcription[0]\n",
    "\n",
    "        #         translation_duration = time.time() - start_translation_time\n",
    "\n",
    "        #         # Save the transcription with a timestamp to the file\n",
    "        #         timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        #         transcription_file.write(f\"[{timestamp}] {transcription_text}\\n\")\n",
    "        #         transcription_file.flush()  # Ensure it's written to the file immediately\n",
    "\n",
    "\n",
    "        #         # Print the transcription\n",
    "        #         print(f\"Transcription ({translation_duration:0.3f}s): {transcription_text}\")\n",
    "\n",
    "        #         # Clear buffer after transcription\n",
    "        #         audio_buffer = np.array([], dtype=np.float32)\n",
    "\n",
    "        #     last_transcription_time = current_time  # Update last transcription time\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopped listening.\")\n",
    "        save_wave(audio_frames, idx=1)\n",
    "        break\n",
    "\n",
    "# Stop and close the stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 28,\n",
       " 'structVersion': 2,\n",
       " 'name': 'Headset (realme Buds Wireless 3 Hands-Free AG Audio) [Loopback]',\n",
       " 'hostApi': 2,\n",
       " 'maxInputChannels': 1,\n",
       " 'maxOutputChannels': 0,\n",
       " 'defaultLowInputLatency': 0.003,\n",
       " 'defaultLowOutputLatency': 0.0,\n",
       " 'defaultHighInputLatency': 0.01,\n",
       " 'defaultHighOutputLatency': 0.0,\n",
       " 'defaultSampleRate': 16000.0,\n",
       " 'isLoopbackDevice': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyaudiowpatch as pyaudio\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "    # Get default WASAPI info\n",
    "wasapi_info = p.get_host_api_info_by_type(pyaudio.paWASAPI)\n",
    "# Get default WASAPI speakers\n",
    "default_speakers = p.get_device_info_by_index(wasapi_info[\"defaultOutputDevice\"])\n",
    "if not default_speakers[\"isLoopbackDevice\"]:\n",
    "    for loopback in p.get_loopback_device_info_generator():\n",
    "        \"\"\"\n",
    "        Try to find loopback device with same name(and [Loopback suffix]).\n",
    "        Unfortunately, this is the most adequate way at the moment.\n",
    "        \"\"\"\n",
    "        if default_speakers[\"name\"] in loopback[\"name\"]:\n",
    "            default_speakers = loopback\n",
    "            break\n",
    "\n",
    "default_speakers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
