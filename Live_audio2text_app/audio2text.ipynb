{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Wave_write.__del__ at 0x000001F15B8D2430>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Asus\\miniconda3\\lib\\wave.py\", line 326, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\Asus\\miniconda3\\lib\\wave.py\", line 444, in close\n",
      "    self._ensure_header_written(0)\n",
      "  File \"c:\\Users\\Asus\\miniconda3\\lib\\wave.py\", line 462, in _ensure_header_written\n",
      "    raise Error('# channels not specified')\n",
      "wave.Error: # channels not specified\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for audio... Speak now.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\miniconda3\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:694: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription (4.233s):  I'm going to buy a croissant and a baguette. I would like a croissant, please. Hello! I'm going to take a croissant and a chocolate bread, please. And with this...\n",
      "Stopped listening.\n"
     ]
    }
   ],
   "source": [
    "# pip install PyAudioWPatch\n",
    "\n",
    "import pyaudiowpatch as pyaudio\n",
    "import time\n",
    "import numpy as np\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "import wave\n",
    "\n",
    "def save_wave(audio_frames, idx):\n",
    "    # Save the recorded audio to a WAV file\n",
    "    wav_file = f\"temp_audio_{idx}.wav\"\n",
    "    with wave.open(wav_file, 'wb') as wf:\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(audio_frames))\n",
    "\n",
    "device='cuda'\n",
    "# Load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to(device)\n",
    "# forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"transcribe\")#for french to french\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"translate\")#for french to english\n",
    "# forced_decoder_ids = None # for english to english\n",
    "\n",
    "\n",
    "# Initialize PyAudio\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "    # Get default WASAPI info\n",
    "wasapi_info = p.get_host_api_info_by_type(pyaudio.paWASAPI)\n",
    "# Get default WASAPI speakers\n",
    "default_speakers = p.get_device_info_by_index(wasapi_info[\"defaultOutputDevice\"])\n",
    "if not default_speakers[\"isLoopbackDevice\"]:\n",
    "    for loopback in p.get_loopback_device_info_generator():\n",
    "        \"\"\"\n",
    "        Try to find loopback device with same name(and [Loopback suffix]).\n",
    "        Unfortunately, this is the most adequate way at the moment.\n",
    "        \"\"\"\n",
    "        if default_speakers[\"name\"] in loopback[\"name\"]:\n",
    "            default_speakers = loopback\n",
    "            break\n",
    "\n",
    "\n",
    "# Settings for recording audio\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000  # Whisper expects 16kHz input\n",
    "CHUNK = 1024  # Number of frames per buffer\n",
    "TRANSCRIPTION_INTERVAL = 20  # Interval for transcription in seconds\n",
    "\n",
    "# Open a stream to record audio\n",
    "\n",
    "stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK, input_device_index=default_speakers[\"index\"])\n",
    "\n",
    "print(\"Listening for audio... Speak now.\")\n",
    "\n",
    "audio_buffer = np.array([], dtype=np.float32)  # Buffer to store accumulated audio\n",
    "last_transcription_time = time.time()  # Initialize the last transcription time\n",
    "audio_frames = []  # Store raw audio frames\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "\n",
    "        # Read a chunk of audio\n",
    "        data = stream.read(CHUNK, exception_on_overflow=False)  # Read a chunk of audio (1024 samples per chunk\n",
    "        audio_frames.append(data)  # Save raw audio data for MP3 conversion\n",
    "        audio_chunk = np.frombuffer(data, np.int16).flatten().astype(np.float32) / 32768.0\n",
    "        \n",
    "        \n",
    "\n",
    "        audio_buffer = np.append(audio_buffer, audio_chunk)  # Accumulate sound in the buffer\n",
    "\n",
    "        # Check if it's time to perform transcription\n",
    "        current_time = time.time()\n",
    "        if current_time - last_transcription_time >= TRANSCRIPTION_INTERVAL:\n",
    "            if audio_buffer.size > 0:  # Ensure there's audio to transcribe\n",
    "\n",
    "                start_translation_time = time.time()\n",
    "\n",
    "                input_features = processor(audio_buffer, sampling_rate=RATE, return_tensors=\"pt\").input_features\n",
    "\n",
    "                # Generate token ids\n",
    "                predicted_ids = model.generate(input_features.to(device), forced_decoder_ids=forced_decoder_ids)\n",
    "\n",
    "                # Decode token ids to text\n",
    "                transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "                translation_duration = time.time() - start_translation_time\n",
    "\n",
    "\n",
    "\n",
    "                # Print the transcription\n",
    "                print(f\"Transcription ({translation_duration:0.3f}s): {transcription[0]}\")\n",
    "\n",
    "                # Clear buffer after transcription\n",
    "                audio_buffer = np.array([], dtype=np.float32)\n",
    "\n",
    "            last_transcription_time = current_time  # Update last transcription time\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopped listening.\")\n",
    "        save_wave(audio_frames, idx=1)\n",
    "        break\n",
    "\n",
    "# Stop and close the stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device-0, audio_chunk:3.0517578125e-05\n",
      "device-0, audio_chunk:0.00128173828125\n",
      "device-0, audio_chunk:0.00152587890625\n",
      "device-0, audio_chunk:0.00213623046875\n",
      "device-0, audio_chunk:0.002166748046875\n",
      "device-0, audio_chunk:0.0025634765625\n",
      "device-0, audio_chunk:0.00213623046875\n",
      "device-0, audio_chunk:0.002471923828125\n",
      "device-0, audio_chunk:0.002777099609375\n",
      "device-0, audio_chunk:0.002227783203125\n",
      "device-1, audio_chunk:0.00146484375\n",
      "device-1, audio_chunk:0.001678466796875\n",
      "device-1, audio_chunk:0.00201416015625\n",
      "device-1, audio_chunk:0.002593994140625\n",
      "device-1, audio_chunk:0.002777099609375\n",
      "device-1, audio_chunk:0.002288818359375\n",
      "device-1, audio_chunk:0.00262451171875\n",
      "device-1, audio_chunk:0.00201416015625\n",
      "device-1, audio_chunk:0.0029296875\n",
      "device-1, audio_chunk:0.0018310546875\n",
      "device-2, audio_chunk:0.000152587890625\n",
      "device-2, audio_chunk:0.000152587890625\n",
      "device-2, audio_chunk:0.000152587890625\n",
      "device-2, audio_chunk:0.00018310546875\n",
      "device-2, audio_chunk:0.000152587890625\n",
      "device-2, audio_chunk:0.000152587890625\n",
      "device-2, audio_chunk:0.000152587890625\n",
      "device-2, audio_chunk:0.000152587890625\n",
      "device-2, audio_chunk:0.000274658203125\n",
      "device-2, audio_chunk:0.00018310546875\n",
      "device-3, audio_chunk:3.0517578125e-05\n",
      "device-3, audio_chunk:3.0517578125e-05\n",
      "device-3, audio_chunk:3.0517578125e-05\n",
      "device-3, audio_chunk:3.0517578125e-05\n",
      "device-3, audio_chunk:3.0517578125e-05\n",
      "device-3, audio_chunk:0.00067138671875\n",
      "device-3, audio_chunk:0.00323486328125\n",
      "device-3, audio_chunk:0.00482177734375\n",
      "device-3, audio_chunk:0.007171630859375\n",
      "device-3, audio_chunk:0.0081787109375\n",
      "device-9, audio_chunk:0.00482177734375\n",
      "device-9, audio_chunk:3.0517578125e-05\n",
      "device-9, audio_chunk:0.0081787109375\n",
      "device-9, audio_chunk:3.0517578125e-05\n",
      "device-9, audio_chunk:0.007171630859375\n",
      "device-9, audio_chunk:0.00482177734375\n",
      "device-9, audio_chunk:0.0081787109375\n",
      "device-9, audio_chunk:3.0517578125e-05\n",
      "device-9, audio_chunk:0.00482177734375\n",
      "device-9, audio_chunk:0.0081787109375\n",
      "device-10, audio_chunk:0.00323486328125\n",
      "device-10, audio_chunk:3.0517578125e-05\n",
      "device-10, audio_chunk:0.00482177734375\n",
      "device-10, audio_chunk:0.007171630859375\n",
      "device-10, audio_chunk:0.0081787109375\n",
      "device-10, audio_chunk:0.00482177734375\n",
      "device-10, audio_chunk:0.0081787109375\n",
      "device-10, audio_chunk:0.00482177734375\n",
      "device-10, audio_chunk:0.0081787109375\n",
      "device-10, audio_chunk:0.00482177734375\n",
      "device-11, audio_chunk:0.0081787109375\n",
      "device-11, audio_chunk:0.00482177734375\n",
      "device-11, audio_chunk:3.0517578125e-05\n",
      "device-11, audio_chunk:0.007171630859375\n",
      "device-11, audio_chunk:3.0517578125e-05\n",
      "device-11, audio_chunk:0.00482177734375\n",
      "device-11, audio_chunk:0.0081787109375\n",
      "device-11, audio_chunk:0.007171630859375\n",
      "device-11, audio_chunk:0.0081787109375\n",
      "device-11, audio_chunk:0.00201416015625\n",
      "device-12, audio_chunk:3.0517578125e-05\n",
      "device-12, audio_chunk:0.00482177734375\n",
      "device-12, audio_chunk:0.007171630859375\n",
      "device-12, audio_chunk:0.0081787109375\n",
      "device-12, audio_chunk:0.00482177734375\n",
      "device-12, audio_chunk:0.0081787109375\n",
      "device-12, audio_chunk:0.00482177734375\n",
      "device-12, audio_chunk:0.0081787109375\n",
      "device-12, audio_chunk:0.00482177734375\n",
      "device-12, audio_chunk:0.0081787109375\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno -9997] Invalid sample rate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m dev \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mget_device_info_by_index(i)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dev[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxInputChannels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 16\u001b[0m    stream \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFORMAT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHANNELS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_per_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHUNK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_device_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m    \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m     18\u001b[0m       data \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mread(CHUNK, exception_on_overflow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Read a chunk of audio (1024 samples per chunk\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\miniconda3\\lib\\site-packages\\pyaudiowpatch\\__init__.py:801\u001b[0m, in \u001b[0;36mPyAudio.open\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    794\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;124;03m    Open a new stream. See constructor for\u001b[39;00m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;124;03m    :py:func:`Stream.__init__` for parameter details.\u001b[39;00m\n\u001b[0;32m    797\u001b[0m \n\u001b[0;32m    798\u001b[0m \u001b[38;5;124;03m    :returns: A new :py:class:`Stream`\u001b[39;00m\n\u001b[0;32m    799\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 801\u001b[0m     stream \u001b[38;5;241m=\u001b[39m Stream(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streams\u001b[38;5;241m.\u001b[39madd(stream)\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "File \u001b[1;32mc:\\Users\\Asus\\miniconda3\\lib\\site-packages\\pyaudiowpatch\\__init__.py:461\u001b[0m, in \u001b[0;36mStream.__init__\u001b[1;34m(self, PA_manager, rate, channels, format, input, output, input_device_index, output_device_index, frames_per_buffer, start, input_host_api_specific_stream_info, output_host_api_specific_stream_info, stream_callback)\u001b[0m\n\u001b[0;32m    458\u001b[0m     arguments[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream_callback\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m stream_callback\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# calling pa.open returns a stream object\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marguments)\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_latency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream\u001b[38;5;241m.\u001b[39minputLatency\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_latency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream\u001b[38;5;241m.\u001b[39moutputLatency\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno -9997] Invalid sample rate"
     ]
    }
   ],
   "source": [
    "import pyaudiowpatch as pyaudio\n",
    "import numpy as np\n",
    "# Settings for recording audio\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000  # Whisper expects 16kHz input\n",
    "CHUNK = 1024  # Number of frames per buffer\n",
    "TRANSCRIPTION_INTERVAL = 20  # Interval for transcription in seconds\n",
    "\n",
    "# Initialize PyAudio\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "for i in range(p.get_device_count()):\n",
    "   dev = p.get_device_info_by_index(i)\n",
    "   if dev['maxInputChannels']>0:\n",
    "      stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK, input_device_index=i)\n",
    "      for _ in range(10):\n",
    "         data = stream.read(CHUNK, exception_on_overflow=False)  # Read a chunk of audio (1024 samples per chunk\n",
    "         audio_chunk = np.frombuffer(data, np.int16).flatten().astype(np.float32) / 32768.0\n",
    "\n",
    "         print(f'device-{i}, audio_chunk:{audio_chunk.max()}')\n",
    "\n",
    "\n",
    "\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A simple example of recording from speakers ('What you hear') using the WASAPI loopback device\"\"\"\n",
    "\n",
    "\n",
    "# Spinner is a helper class that is in the same examples folder.\n",
    "# It is optional, you can safely delete the code associated with it.\n",
    "\n",
    "import pyaudiowpatch as pyaudio\n",
    "import time\n",
    "import wave\n",
    "\n",
    "DURATION = 5.0\n",
    "CHUNK_SIZE = 512\n",
    "\n",
    "filename = \"loopback_record.wav\"\n",
    "    \n",
    "    \n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "wave_file = wave.open(filename, 'wb')\n",
    "wave_file.setnchannels(1)\n",
    "wave_file.setsampwidth(pyaudio.get_sample_size(pyaudio.paInt16))\n",
    "wave_file.setframerate(int(default_speakers[\"defaultSampleRate\"]))\n",
    "\n",
    "def callback(in_data, frame_count, time_info, status):\n",
    "    \"\"\"Write frames and return PA flag\"\"\"\n",
    "    wave_file.writeframes(in_data)\n",
    "    return (in_data, pyaudio.paContinue)\n",
    "\n",
    "with p.open(format=pyaudio.paInt16,\n",
    "        channels=default_speakers[\"maxInputChannels\"],\n",
    "        rate=int(default_speakers[\"defaultSampleRate\"]),\n",
    "        frames_per_buffer=CHUNK_SIZE,\n",
    "        input=True,\n",
    "        input_device_index=default_speakers[\"index\"],\n",
    "        stream_callback=callback\n",
    ") as stream:\n",
    "    \"\"\"\n",
    "    Opena PA stream via context manager.\n",
    "    After leaving the context, everything will\n",
    "    be correctly closed(Stream, PyAudio manager)            \n",
    "    \"\"\"\n",
    "    time.sleep(DURATION) # Blocking execution while playing\n",
    "\n",
    "wave_file.close()\n",
    "\n",
    "# stream.stop_stream()\n",
    "# stream.close()\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 28,\n",
       " 'structVersion': 2,\n",
       " 'name': 'Headset (realme Buds Wireless 3 Hands-Free AG Audio) [Loopback]',\n",
       " 'hostApi': 2,\n",
       " 'maxInputChannels': 1,\n",
       " 'maxOutputChannels': 0,\n",
       " 'defaultLowInputLatency': 0.003,\n",
       " 'defaultLowOutputLatency': 0.0,\n",
       " 'defaultHighInputLatency': 0.01,\n",
       " 'defaultHighOutputLatency': 0.0,\n",
       " 'defaultSampleRate': 16000.0,\n",
       " 'isLoopbackDevice': True}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_speakers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
