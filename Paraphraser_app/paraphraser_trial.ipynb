{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n",
    "\n",
    "def paraphrase(\n",
    "    question,\n",
    "    num_beams=5,\n",
    "    num_beam_groups=5,\n",
    "    num_return_sequences=1,\n",
    "    repetition_penalty=10.0,\n",
    "    diversity_penalty=3.0,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=0.7,\n",
    "    max_length=128\n",
    "):\n",
    "    input_ids = tokenizer(\n",
    "        f'paraphrase: {question}',\n",
    "        return_tensors=\"pt\", padding=\"longest\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids.to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n",
    "        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_beams=num_beams, num_beam_groups=num_beam_groups,\n",
    "        max_length=max_length, diversity_penalty=diversity_penalty\n",
    "    )\n",
    "\n",
    "    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Founded in 2016, Hugging Face is an innovative AI company that has become a leading player in natural language processing (NLP) and machine learning tools. Its main focus was on developing open-source models and libraries for NLP tasks, with primarily non-MIT proprietary models available such as the Transformers library. These models have established standards that are used extensively in industry applications.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = \"\"\"  Hugging Face is an innovative AI company that has become a leading platform for natural language processing (NLP) and machine learning tools. Founded in 2016, it started as a chatbot app but soon pivoted to focus on developing open-source models and libraries for NLP tasks. Hugging Face is best known for its **Transformers library**, which provides pre-trained models for tasks like text classification, translation, summarization, and question-answering. These models, especially BERT, GPT, and T5, have become industry standards for NLP applications.\n",
    "\n",
    "The platform's mission is to democratize AI by making state-of-the-art models accessible to researchers, developers, and organizations. Hugging Face provides tools that simplify the development, training, and deployment of machine learning models. Its **Model Hub** allows users to easily share and access thousands of pre-trained models in various domains, from NLP to vision and audio tasks.\n",
    "\n",
    "Hugging Face also supports **AutoML**, enabling users to fine-tune models on their own data without deep expertise in machine learning. With a vibrant open-source community, Hugging Face is at the forefront of AI innovation, providing powerful, user-friendly tools that fuel advancements in machine learning and AI research across industries. \"\"\"\n",
    "\n",
    "paraphrase(paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Paragraph:\n",
      "   Hugging Face is an innovative AI company that has become a leading platform for natural language processing (NLP) and machine learning tools. Founded in 2016, it started as a chatbot app but soon pivoted to focus on developing open-source models and libraries for NLP tasks. Hugging Face is best known for its **Transformers library**, which provides pre-trained models for tasks like text classification, translation, summarization, and question-answering. These models, especially BERT, GPT, and T5, have become industry standards for NLP applications.\n",
      "\n",
      "The platform's mission is to democratize AI by making state-of-the-art models accessible to researchers, developers, and organizations. Hugging Face provides tools that simplify the development, training, and deployment of machine learning models. Its **Model Hub** allows users to easily share and access thousands of pre-trained models in various domains, from NLP to vision and audio tasks.\n",
      "\n",
      "Hugging Face also supports **AutoML**, enabling users to fine-tune models on their own data without deep expertise in machine learning. With a vibrant open-source community, Hugging Face is at the forefront of AI innovation, providing powerful, user-friendly tools that fuel advancements in machine learning and AI research across industries. \n",
      "\n",
      "Paraphrased Paragraph: \n",
      " Hugging Face, an inventive AI company, has emerged as a prominent leader in natural language processing (NLP) and machine learning tools. It was founded in 2016 as a chatbot app, but quickly turned its attention towards open-source models and libraries for NLP tasks. Hugging Face's pre-trained models for various tasks, including text classification, translation, summarization, and question-answering, are primarily found in the transformers library. Industry standards for NLP applications have been established with the use of BERT, GPT, and T5. The goal of the site is to make AI more accessible to researchers, developers, and organizations by making advanced models available. Hugging Face's suite of tools simplifies the process of creating, training, and deploying machine learning models. Users can use the **Model Hub** to share and access thousands of pre-trained models in a variety of fields, including NLP, vision, and audio. Hugging Face supports **AutoML**, allowing users to fine-tune models on their own data without significant experience with machine learning. Hugging Face, a community of open-source software, is at the forefront of AI innovation and offers practical tools that promote advancements in machine learning and AI research across diverse fields.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the paragraph into sentences\n",
    "sentences = sent_tokenize(paragraph)\n",
    "\n",
    "paraphrased_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    paraphrased_result = paraphrase(sentence, num_beams=2, num_beam_groups=2, num_return_sequences=1)\n",
    "    paraphrased_sentences.append(paraphrased_result[0])  # Take the first paraphrased output\n",
    "\n",
    "# Join the paraphrased sentences back together\n",
    "paraphrased_paragraph = \" \".join(paraphrased_sentences)\n",
    "\n",
    "print(\"Original Paragraph:\\n\", paragraph)\n",
    "print(\"\\nParaphrased Paragraph: \\n\", paraphrased_paragraph)\n",
    "len(sentences),len(paraphrased_sentences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
