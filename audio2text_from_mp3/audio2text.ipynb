{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing mp3 file to numpy requires the FFmpeg. Follow the steps in this link to have it installed. [(How to install FFmpeg in windows)](https://www.wikihow.com/Install-FFmpeg-on-Windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import  pipeline,AutoModelForSpeechSeq2Seq,AutoProcessor\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "pipe  = pipeline(\"automatic-speech-recognition\",\n",
    "                    \"openai/whisper-small\", \n",
    "                    chunk_length_s=30,\n",
    "                    stride_length_s=5,\n",
    "                    return_timestamps=True,\n",
    "                    device=device, \n",
    "                    generate_kwargs = {\"language\": 'English', \"task\": \"translate\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You have passed task=translate, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=translate.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 20.52)--> So first, we'll start by importing the essential libraries such as PyAudioWpatch.\n",
      "(20.52, 24.88)--> It's for capturing the speaker output on Windows, especially.\n",
      "(24.88, 28.0)--> We have Torch for running the Ushper model here.\n",
      "(28.0, 34.0)--> And few other like NumPy, Pond, SciPy, and Matplotlib,\n",
      "(34.0, 36.0)--> for just plotting purposes.\n",
      "(40.0, 42.0)--> Next come one of the important part,\n",
      "(42.0, 45.0)--> that how we are going to set up our microphone.\n",
      "(45.0, 55.0)--> Because in the end our goal is that the microphone of our device will capture the audio that we speak,\n",
      "(55.0, 61.0)--> it convert into some signal processing data, we pass it through the whisper model and we should get the transcription for it.\n",
      "(61.0, 67.92)--> But for that first we have to set up the microphone and we are going to use this Pi Audio W patch library.\n",
      "(67.92, 71.04)--> You can also use just Pi Audio library.\n",
      "(71.04, 75.44)--> Why I am using this Windows patch is because it can help you\n",
      "(75.44, 77.92)--> to get Lumback devices.\n",
      "(77.92, 78.92)--> So what does that mean?\n",
      "(78.92, 83.52)--> It is you can, if you just want to get the recording\n",
      "(83.52, 85.0)--> from the microphone,\n",
      "(85.0, 87.0)--> it's okay, you can use Pi Audio.\n",
      "(87.0, 92.0)--> But what if you want to get the recording of something playing in your system?\n",
      "(92.0, 97.0)--> In that case, you need a loopback device that can feed the speaker,\n",
      "(97.0, 101.0)--> what is the output of the speaker back as an input.\n",
      "(101.0, 103.0)--> So that's why we need this loopback device.\n",
      "(103.0, 105.88)--> And to enable this in a Windows system,\n",
      "(105.88, 113.16)--> you need this Pi Audio W patch. It's basically an wrapper around a Pi Audio library. So what\n",
      "(113.16, 119.96)--> I do is I first list all the devices that I have available to me. Here you see, I have\n",
      "(119.96, 128.28)--> total how many, three, six, total six devices who have input channels greater than zero.\n",
      "(128.28, 129.44)--> So what does that mean?\n",
      "(129.44, 135.04)--> Is they can be used as a microphone or an input device\n",
      "(135.04, 137.28)--> for this case.\n",
      "(137.28, 140.52)--> I have this microphone sound mapper headset\n",
      "(140.52, 142.92)--> that I am currently using my headset.\n",
      "(142.92, 146.88)--> I have two values of that, two such instances.\n",
      "(151.52, 157.68)--> And this one you see this loopback device it's the speaker of my microphone. Also this one\n",
      "(157.68, 167.02)--> is for my actual system. If I want to display something, suppose for example if I am\n",
      "(167.02, 172.54)--> playing any video on YouTube and I want to use the audio of that YouTube video\n",
      "(172.54, 176.8)--> as my input to the model then I need to use something like this. Alright so\n",
      "(176.8, 179.34)--> that's about it but in this video we are not going to talk about the loopback\n",
      "(179.34, 187.6)--> device this is just an if you want you can try it on your own. I'm just going to use this normal microphone.\n",
      "(188.64, 191.2)--> For that I'm using this first index one.\n",
      "(194.96, 202.08)--> I would like you to for now ignore this part 25 because this was on my another device where I\n",
      "(202.08, 207.36)--> had to use a different index. This index may change depending upon your system.\n",
      "(207.36, 209.24)--> But you have to try like couple of times\n",
      "(209.24, 211.12)--> that which one works, which one doesn't work.\n",
      "(211.12, 212.72)--> And to see that which one works,\n",
      "(212.72, 215.68)--> you see you have index numbers here,\n",
      "(216.62, 219.6)--> this first ones, you put plug them here,\n",
      "(219.6, 221.24)--> input device equal to one.\n",
      "(222.28, 232.32)--> Then we do, then we get the info of that device that will be our speaker\n",
      "(232.32, 236.8)--> with which we will record the audio.\n",
      "(236.8, 241.92)--> I also have to fix up the sampling rate at which the speaker needs to record the audio\n",
      "(241.92, 246.12)--> and then this is the stream object that I create.\n",
      "(246.12, 253.76)--> This will help me record the audio.\n",
      "(253.76, 265.0)--> Then with the stream.read, this is the main part where it reads 10 second of audio.\n",
      "(266.68, 271.68)--> So here rate depends upon your speaker.\n",
      "(272.24, 277.24)--> For example, here if I write rate for this,\n",
      "(277.8, 282.8)--> it's 44,000, 44,100 hertz is the sampling rate.\n",
      "(285.28, 291.0)--> That means in one second it collects 44,000 samples and I am going to do that for 10 seconds.\n",
      "(291.0, 406.0)--> So it is going to collect total 44,000 into 10 times number of samples. And all this data that is all the data that has been captured by stream are in binary. I'm going to use the same method as the other one. Hello everyone and welcome back to my channel. Haji Haji, Bur Bur Bur, Jub Bur Jub, Do Do Di Do Di Do Di Da Da. I'm going to start with the first one. I'm going to start with the second one. I'm going to start with the third one.\n",
      "(406.0, 408.0)--> I'm going to start with the fourth one.\n",
      "(408.0, 410.0)--> I'm going to start with the fourth one.\n",
      "(410.0, 412.0)--> I'm going to start with the fourth one.\n",
      "(412.0, 414.0)--> I'm going to start with the fourth one.\n",
      "(414.0, 416.0)--> I'm going to start with the fourth one.\n",
      "(416.0, 418.0)--> I'm going to start with the fourth one.\n",
      "(418.0, 420.0)--> I'm going to start with the fourth one.\n",
      "(420.0, 422.0)--> I'm going to start with the fourth one.\n",
      "(422.0, 424.0)--> I'm going to start with the fourth one.\n",
      "(424.0, 426.0)--> I'm going to start with the fourth one. I'm going to start with the second one. I'm going to start with the third one.\n",
      "(426.0, 428.0)--> I'm going to start with the fourth one.\n",
      "(428.0, 430.0)--> I'm going to start with the fourth one.\n",
      "(430.0, 432.0)--> I'm going to start with the fourth one.\n",
      "(432.0, 434.0)--> I'm going to start with the fourth one.\n",
      "(434.0, 436.0)--> I'm going to start with the fourth one.\n",
      "(436.0, 438.0)--> I'm going to start with the fourth one.\n",
      "(438.0, 440.0)--> I'm going to start with the fourth one.\n",
      "(440.0, 442.0)--> I'm going to start with the fourth one.\n",
      "(442.0, 444.0)--> I'm going to start with the fourth one.\n",
      "(444.0, 447.0)--> I'm going to start with the fourth one. I'm going to start with the first one.\n",
      "(447.0, 450.0)--> I'm going to start with the first one.\n",
      "(450.0, 452.0)--> I'm going to start with the first one.\n",
      "(452.0, 454.0)--> I'm going to start with the first one.\n",
      "(454.0, 456.0)--> I'm going to start with the first one.\n",
      "(456.0, 458.0)--> I'm going to start with the first one.\n",
      "(458.0, 460.0)--> I'm going to start with the first one.\n",
      "(460.0, 462.0)--> I'm going to start with the first one.\n",
      "(462.0, 464.0)--> I'm going to start with the first one.\n",
      "(464.0, 466.0)--> I'm going to start with the first one. I'm going to start you how to do it. I'm going to show you how to do it. I'm going to show you how to do it.\n",
      "(466.0, 468.0)--> I'm going to show you how to do it.\n",
      "(468.0, 470.0)--> I'm going to show you how to do it.\n",
      "(470.0, 472.0)--> I'm going to show you how to do it.\n",
      "(472.0, 474.0)--> I'm going to show you how to do it.\n",
      "(474.0, 476.0)--> I'm going to show you how to do it.\n",
      "(476.0, 478.0)--> I'm going to show you how to do it.\n",
      "(478.0, 480.0)--> I'm going to show you how to do it.\n",
      "(480.0, 482.0)--> I'm going to show you how to do it.\n",
      "(482.0, 484.0)--> I'm going to show you how to do it.\n",
      "(484.0, 506.0)--> I'm going to show you how to do it. I'm going to show you how to do it. Thank you. I'm going to start with the first one. I'm going to start with the second one. I'm going to start with the third one.\n",
      "(506.0, 508.0)--> I'm going to start with the fourth one.\n",
      "(508.0, 510.0)--> I'm going to start with the fourth one.\n",
      "(510.0, 512.0)--> I'm going to start with the fourth one.\n",
      "(512.0, 514.0)--> I'm going to start with the fourth one.\n",
      "(514.0, 516.0)--> I'm going to start with the fourth one.\n",
      "(516.0, 518.0)--> I'm going to start with the fourth one.\n",
      "(518.0, 520.0)--> I'm going to start with the fourth one.\n",
      "(520.0, 522.0)--> I'm going to start with the fourth one.\n",
      "(522.0, 524.0)--> I'm going to start with the fourth one.\n",
      "(524.0, 583.36)--> I'm going to start with the fourth one. I'm going to start with the fourth one. So, here the data represents raw audio data captured from the selected input device.\n",
      "(583.36, 585.88)--> It's in the form of byte string.\n",
      "(585.88, 589.98)--> So if I display my variable,\n",
      "(591.4, 593.32)--> all the variables I have in this Jupyter normal,\n",
      "(593.32, 596.24)--> you see this data, it's in the bytes format\n",
      "(596.24, 597.98)--> and it looks something like this.\n",
      "(600.48, 602.56)--> Then I have to convert it into NumPy.\n",
      "(602.56, 608.0)--> For that, I use this np.from before. I convert it into numpy for that I use this np.from before I convert it into int16.\n",
      "(608.0, 614.0)--> I flatten it then I convert it into float16 because I want to do the division.\n",
      "(614.0, 628.92)--> And you know right now at this part my data has int16 format that means the maximum value is 23768 in that data format but I want to\n",
      "(628.92, 635.84)--> normalize it to 0 to 1 for that I divided by 32768 so that this whole data is in the\n",
      "(635.84, 654.0)--> range of 0 to 1 because that's what the whisper model expects. Hello everyone and welcome back to my channel.\n",
      "(654.0, 666.8)--> If I plot this whole thing, if I plot the audio channel, you see something like this that I spoke in the middle and yeah this is the speech activity.\n",
      "(680.88, 685.68)--> So if you are able to see a waveform like like this, you are ready to feed audio into your\n",
      "(685.68, 700.0)--> whisper model. Next diving into the whisper model, let us understand how the whisper processor walk.\n",
      "(701.04, 706.48)--> The input is first converted into a mel male spectrogram using Fourier transform.\n",
      "(706.48, 713.64)--> So here you see this input is this one, this particular thing, but then the whisper processor\n",
      "(713.64, 721.8)--> here, it helps us to convert this one dimensional time series into something called as a male\n",
      "(721.8, 728.36)--> spectrogram.\n",
      "(731.44, 736.2)--> But before that, one thing we have to note that the whisper model except 16,000 sampling rate,\n",
      "(736.2, 737.64)--> it doesn't expect more or less.\n",
      "(737.64, 740.64)--> This is what it strained on and this is what it expects.\n",
      "(740.64, 744.4)--> So before feeding our this raw data into the\n",
      "(744.4, 747.86)--> Windows whisper processor, we need to\n",
      "(747.86, 752.04)--> resample it to have this particular sampling rate.\n",
      "(752.04, 759.08)--> And we do this using this very simple resampling procedure from scipy.\n",
      "(759.08, 769.0)--> Then this processor that we have initialized the processor object, we pass this audio chunk, the sampling rate\n",
      "(769.0, 775.96)--> which is the whisper sampling rate of 16,000 and it returns as a PyTorch tensor and the\n",
      "(775.96, 778.74)--> device will choose to be the GPU.\n",
      "(778.74, 783.84)--> And then we see this input feature is basically an array.\n",
      "(783.84, 792.4)--> The first one is the batch dimension. The second is 80 number of\n",
      "(792.4, 820.04)--> frequencies and this 3000 is the number of discretization that you have done. If I plot it using IAM so, I think we can do it. So this was my audio\n",
      "(820.04, 826.88)--> data and here you see you have in the starting you do not have anything at the ending you\n",
      "(826.88, 838.52)--> do not have anything and because the input size to the whisper model is of maximum 30\n",
      "(838.52, 848.0)--> second and yeah for 30 second it divide that into 30,000 chunks. That's why, because we have a 10 second,\n",
      "(848.0, 852.0)--> here you see.\n",
      "(852.0, 856.0)--> Yeah.\n",
      "(856.0, 860.0)--> So here we have only 10 second chunk and only we are speaking between this time\n",
      "(860.0, 864.0)--> step. So that part is, you know, you see some activity there and for the rest\n",
      "(864.0, 865.0)--> you don't see any activity.\n",
      "(865.0, 871.0)--> If you give anything less than, you know, 30 seconds, it will just pad them with zeros.\n",
      "(875.0, 880.0)--> All right. So yeah, that was about it.\n",
      "(880.0, 883.0)--> And next, it's very simple. We already got the feature.\n",
      "(883.0, 886.72)--> Then we pass it through our model,\n",
      "(886.72, 893.4)--> this model that we spare for conditional generation. We pass it through the model.generate function,\n",
      "(893.4, 897.8)--> the input feature. We give it what task we want it to perform, we want it to transcribe\n",
      "(897.8, 905.0)--> and the language is English. Whatever the output we got, we got the basically IDs.\n",
      "(905.0, 908.12)--> Then we have to use the tokenization.\n",
      "(908.12, 909.2)--> We got the tokens.\n",
      "(909.2, 911.34)--> Then we have to convert the tokens to word.\n",
      "(911.34, 914.08)--> For that, we are using the processor.batch decode.\n",
      "(916.84, 919.68)--> After we perform this, we get this transcription.\n",
      "(919.68, 923.22)--> So the raw transcription looks something like this.\n",
      "(923.22, 926.0)--> The first are some like startup transcript,\n",
      "(926.0, 931.0)--> then the language of the spoken audio,\n",
      "(931.52, 933.16)--> then what task you want to do.\n",
      "(933.16, 935.4)--> Either translate or transcribe.\n",
      "(935.4, 938.4)--> Then we specify that we do not need timestamp.\n",
      "(938.4, 940.16)--> You can also enable timestamp,\n",
      "(940.16, 942.24)--> but in this case we are not requiring that.\n",
      "(942.24, 944.16)--> And then hello everyone and welcome back to my channel.\n",
      "(944.16, 947.36)--> Today we are going to talk about nothing. is what I said during the in the audio.\n",
      "(947.92, 1016.44)--> This is the whole process of you know Ushper how it works. I'm going to run this entire code in a loop that while true and it will\n",
      "(1016.44, 1020.44)--> only stop if we interrupt using the keyboard.\n",
      "(1020.44, 1025.0)--> And yeah, this part I just got, which is they don't mean anything.\n",
      "(1025.0, 1027.6)--> For example, I can just comment them out here.\n",
      "(1027.6, 1030.6)--> This part.\n",
      "(1030.6, 1034.6)--> This part only this part is what is important.\n",
      "(1034.6, 1073.56)--> This is just like I want to estimate some time and that's it. After that, I am going to run this whole process in a loop.\n",
      "(1073.56, 1078.64)--> So what I am doing first is I am just reading the data stream from my speaker object and\n",
      "(1078.64, 1080.76)--> I am converting into NumPy array.\n",
      "(1080.76, 1085.7)--> Then I am resampling to fit in the whisper sampling rate.\n",
      "(1085.7, 1091.58)--> This is just an additional audio frame because I want to record what I am speaking.\n",
      "(1091.58, 1103.54)--> Next is our more.\n",
      "(1103.54, 1106.96)--> Next I use the whisper processor.\n",
      "(1106.96, 1112.84)--> I pass the audio chunk for the length.\n",
      "(1112.84, 1117.08)--> And it gives me the input feature, which is the log mail spectrum, the heat map that\n",
      "(1117.08, 1118.08)--> we saw.\n",
      "(1118.08, 1156.0)--> I pass it through the model. I pass it through the model, I get my predicted IDs, then I decode those predicted IDs to So for now let's run it.\n",
      "(1156.0, 1159.0)--> Hello everyone and welcome back to my channel.\n",
      "(1159.0, 1165.0)--> Today we are going to see how to do whisper training using Lora.\n",
      "(1169.32, 1170.48)--> Yeah, when I don't speak anything,\n",
      "(1170.48, 1174.56)--> it starts hallucinating, but there is a way to this.\n",
      "(1175.6, 1177.78)--> What we can do is here we can increase.\n",
      "(1179.0, 1181.08)--> The transcription interval is one second I have put.\n",
      "(1181.08, 1189.04)--> Maybe let's put 10 second and see how it performs.\n",
      "(1193.28, 1196.88)--> Hello everyone and welcome back to my channel. Today we are going to talk about how the whisper model works in real time speech recognition.\n",
      "(1200.08, 1208.4)--> See it did its job pretty damn well and And yeah, that was all about this video.\n",
      "(1208.4, 1212.4)--> I have also created a web app dedicating the same thing.\n",
      "(1212.4, 1214.32)--> I have made it available on GitHub.\n",
      "(1214.32, 1237.44)--> If you want, you can check them out as well. Thank you.\n"
     ]
    }
   ],
   "source": [
    "transcription = pipe(r\"C:\\Users\\User\\Desktop\\Gen-AI-Mini-Projects\\audio2text_from_mp3\\output_audio.mp3\" )\n",
    "\n",
    "\n",
    "\n",
    "formatted_lyrics = \"\"\n",
    "for line in transcription['chunks']:\n",
    "    text = line[\"text\"]\n",
    "    ts = line[\"timestamp\"]\n",
    "    formatted_lyrics += f\"{ts}-->{text}\\n\"\n",
    "\n",
    "print(formatted_lyrics.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distil WISPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "model_id = \"distil-whisper/distil-small.en\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    return_timestamps=True,\n",
    "    chunk_length_s=15,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 3.0)--> What's your interest in it, Mr. Wayne?\n",
      "(3.0, 7.0)--> I want to borrow it for uh, spill-lunking.\n",
      "(7.0, 9.0)--> Spillunking?\n",
      "(9.0, 11.0)--> Yeah, you know, cave diving.\n",
      "(12.0, 14.44)--> Expecting to run into much much gunfire on these caves.\n"
     ]
    }
   ],
   "source": [
    "transcription = pipe(r\"F:\\Gen-AI-Mini-Projects\\audio2text_from_mp3\\english_audio.mp3\" )\n",
    "\n",
    "\n",
    "\n",
    "formatted_lyrics = \"\"\n",
    "for line in transcription['chunks']:\n",
    "    text = line[\"text\"]\n",
    "    ts = line[\"timestamp\"]\n",
    "    formatted_lyrics += f\"{ts}-->{text}\\n\"\n",
    "\n",
    "print(formatted_lyrics.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
