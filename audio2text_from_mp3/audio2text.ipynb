{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing mp3 file to numpy requires the FFmpeg. Follow the steps in this link to have it installed. [(How to install FFmpeg in windows)](https://www.wikihow.com/Install-FFmpeg-on-Windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import  pipeline,AutoModelForSpeechSeq2Seq,AutoProcessor\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "pipe  = pipeline(\"automatic-speech-recognition\",\n",
    "                    \"openai/whisper-small\", \n",
    "                    chunk_length_s=30,\n",
    "                    stride_length_s=5,\n",
    "                    return_timestamps=True,\n",
    "                    device=device, \n",
    "                    generate_kwargs = {\"language\": 'English', \"task\": \"translate\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You have passed task=translate, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=translate.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 8.44)--> Hello everyone and welcome back to my channel.\n",
      "(8.44, 16.12)--> In this video, we are going to talk about the whisper model and we are going much deeper\n",
      "(16.12, 17.8)--> that we went in the previous video.\n",
      "(17.8, 22.64)--> By the way, if you haven't checked out the previous video, I will put a link for them\n",
      "(22.64, 29.16)--> in the description below.\n",
      "(33.0, 36.4)--> So in this video, our goal is to understand much deeper level that what is the whisper model\n",
      "(36.4, 38.68)--> and how does it work, but in a very practical way.\n",
      "(38.68, 42.08)--> There won't be any maths involved in it.\n",
      "(42.08, 50.24)--> And how we can fine tune this whisper model on our data. For example, we have some\n",
      "(50.24, 55.2)--> other language to train on or different type of data set. For this particular case, I am\n",
      "(55.2, 68.0)--> taking an example of air traffic control data set. Why is this data I'm talking about? Let me play one sample.\n",
      "(68.0, 74.0)--> Okay, go India folks who are from MLContact Praharaydar 127.580 by\n",
      "(74.0, 81.0)--> You see, most of them are in English, like all of them are in English and the language is a little different.\n",
      "(81.0, 85.2)--> Here they talk about Oscar, Kilo, India, like there are some\n",
      "(85.2, 91.52)--> code words they talk when they are doing the pilot is talking to the air traffic controller.\n",
      "(92.48, 102.24)--> And this data, the whisper model is not aware of. To check that, we are first going to pass in a,\n",
      "(103.6, 106.56)--> so first of all, we are going to download this dataset.\n",
      "(106.56, 108.6)--> How you download this dataset is pretty easy.\n",
      "(108.6, 114.4)--> You go here, you go to the data, just download this paracet file and then I will show you\n",
      "(114.4, 130.0)--> how to extract this paracet file in your, later on I will show you how to extract this Parakit file to get the dataset.\n",
      "(130.0, 135.84)--> All right.\n",
      "(135.84, 139.88)--> And the main problem here is the whisper model doesn't understand this code word.\n",
      "(139.88, 146.2)--> So it doesn't give us a very good transcription for this audio files.\n",
      "(146.2, 147.68)--> And that's what we want to eradicate.\n",
      "(147.68, 151.4)--> We want to fine tune the whisper model on this particular dataset.\n",
      "(151.4, 159.56)--> So our whisper model can generate better or more accurate transcription for air traffic\n",
      "(159.56, 167.28)--> control. And this is going to be the first part of the video.\n",
      "(167.28, 172.0)--> So in this particular part, we are just going to understand more about the Usper model,\n",
      "(172.0, 180.52)--> the every part of Usper and we'll do a single forward and backward pass and update the weights\n",
      "(180.52, 187.76)--> using a single sample to understand that how everything works.\n",
      "(187.76, 200.32)--> So with that said let's move on and go to the powerpoint.\n",
      "(200.32, 204.28)--> So this is the overall structure of the whisper model.\n",
      "(204.28, 208.76)--> So what it does, this is the encoder part and this is the decoder part.\n",
      "(208.76, 213.38)--> This decoder part is very similar to chatGPT where it does, where it predicts the next\n",
      "(213.38, 214.88)--> word auto-regressible.\n",
      "(214.88, 220.64)--> However, except for the input tokens, these are the input tokens which is the start of\n",
      "(220.64, 222.6)--> the sentence, the language and the task.\n",
      "(222.6, 225.36)--> It can be transcribed or translate.\n",
      "(225.36, 235.88)--> And then it starts generating the word. But along with that, it is also conditioned on\n",
      "(235.88, 242.56)--> the speech. But it cannot directly understand speech for so far. First, we have to convert\n",
      "(242.56, 247.84)--> the speech into a log mail spectrogram, which looks something like this, then it is passed into the encoder\n",
      "(247.84, 251.52)--> block. All right, so yeah, that's about it.\n",
      "(253.52, 259.36)--> And now we'll see the three main parts of the whisper model. First is the\n",
      "(259.36, 267.4)--> tokenizer. Next is the feature extractor. And after after that we have the actual model, the wishful model.\n",
      "(267.4, 271.88)--> So the feature extractor does this part.\n",
      "(271.88, 279.52)--> This part, it takes the audio signal and converts it into a log mail spectrum and the tokenizer\n",
      "(279.52, 290.32)--> does this part. part where we have the text and the tokenizer convert them into a particular numbers.\n",
      "(290.32, 292.12)--> That's it.\n",
      "(292.12, 295.68)--> When we are doing the inference, we just need the feature extractor and model, we do not\n",
      "(295.68, 297.8)--> need the tokenizer.\n",
      "(297.8, 307.6)--> Here also, you can more modify it by mentioning the language, the language of the transcription, like which\n",
      "(307.6, 312.84)--> language you are focusing on and you are transcribing your translation.\n",
      "(312.84, 316.28)--> And to start with, let's see what is this feature extractor.\n",
      "(316.28, 324.32)--> First we use from the transformer, first we import this whisper feature extractor.\n",
      "(324.32, 328.02)--> I'm using the OpenAI small model.\n",
      "(328.02, 331.98)--> This is an example of the, you know, what has been said.\n",
      "(331.98, 338.46)--> This is what has been said in the speech.\n",
      "(338.46, 343.9)--> And if you plot the speech just in Python, you see some, this web form.\n",
      "(343.9, 346.0)--> One thing to note here is the sampling rate.\n",
      "(346.0, 350.12)--> The sampling rate should be 16,000 because this is the sampling rate whisper model is\n",
      "(350.12, 351.12)--> straight on.\n",
      "(351.12, 356.72)--> If your recorded audio is not of this sampling rate, then you have to down sample or up sample\n",
      "(356.72, 358.28)--> based on.\n",
      "(358.28, 370.0)--> And on the x-axis, you see this is not the time axis, but number of samples. So you have total 120,000 samples and the sampling rate is 16,000.\n",
      "(370.0, 375.24)--> That means there is around 7 to 8 second of data present here.\n",
      "(375.24, 379.68)--> And after that, this is the mail spectrogram or log mail spectrogram that you obtained\n",
      "(379.68, 381.88)--> from the feature extractor.\n",
      "(381.88, 385.12)--> You pass this particular data into the feature extractor\n",
      "(385.12, 387.56)--> and you get this log mail spectrogram.\n",
      "(387.56, 390.2)--> And here you see, until the eight second,\n",
      "(390.2, 392.0)--> we have some activity, but after that,\n",
      "(392.0, 393.48)--> it's just padded with zero.\n",
      "(394.84, 397.76)--> The input to the encoder part is fixed,\n",
      "(397.76, 399.36)--> which is for 30 second.\n",
      "(399.36, 402.2)--> If your, this audio is more than 30 second,\n",
      "(402.2, 404.4)--> then it will be truncated, it will be cut out.\n",
      "(406.16, 414.08)--> Next, we'll see the tokenizer part. So this is the text we have, Oscar, Kilo, Papa, Mike,\n",
      "(414.08, 422.0)--> Bravo, you know, in the air traffic controller, lingo. Then what we do is we add some extra tokens,\n",
      "(422.0, 425.64)--> which is the startup transcript, the language, then the\n",
      "(425.64, 430.24)--> task, transcribe or translate, and then whether you want the time steps or not.\n",
      "(430.24, 433.54)--> And finally, the end of text.\n",
      "(433.54, 439.0)--> And then this, you know, the tokenizer model converts them into some numbers.\n",
      "(439.0, 446.64)--> You see, from the numbers mentioned bold are are this text and the rest are the starting token,\n",
      "(446.64, 452.88)--> the fixed token, the special token events. Now let's have a look at our whisper model.\n",
      "(452.88, 475.8)--> So this is our whisper model. We pass in the log mail spectrogram as input and we get as output and we get as output the tokens, the token IDs.\n",
      "(475.8, 491.08)--> This is the whisper model, but there is one thing to change here. output right now, this output keeps on changing depending upon you know for a suppose you are\n",
      "(491.08, 499.08)--> using 30 second of audio for 30 second of video audio you can also have like 50 50 tokens at the\n",
      "(499.08, 506.76)--> output and at some cases 70 tokens some cases 100 tokens That's why we need to fix the size of output as well.\n",
      "(507.92, 510.52)--> That's what the size of the output is not fixed right now,\n",
      "(510.52, 512.46)--> but we want to fix it.\n",
      "(512.46, 514.16)--> So for that, for the Whisper model,\n",
      "(514.16, 518.0)--> they selected 448 to be a fixed output size.\n",
      "(518.0, 523.0)--> And what they did, they padded the rest with minus 100.\n",
      "(524.8, 531.34)--> This minus 100 is important because the whisper model only whenever it\n",
      "(531.34, 539.74)--> sees minus 100 it ignores that part. And now we will see the training of whisper.\n",
      "(539.74, 543.86)--> During the training of whisper pass it in the log mail spectrogram we get some certain\n",
      "(543.86, 548.74)--> output from it and then there is the actual target, the real target.\n",
      "(548.74, 555.18)--> Then we compare these two using a cross entropy loss, this between them and whatever we found\n",
      "(555.18, 561.9)--> we update the weights of the whisper.\n",
      "(561.9, 572.96)--> That was all about the whisper model. Now we will go ahead and see how to perform that in the Jupyter notebook.\n",
      "(572.96, 585.92)--> So in the Jupyter notebook, in Jupyter notebook, first I would request it to install all these libraries.\n",
      "(585.92, 591.64)--> They will be required in the later part of this series to evaluate the model.\n",
      "(591.64, 594.08)--> But right now, for this case, we don't need it.\n",
      "(594.08, 618.52)--> We need its dataset part, datasets. So, as I told you that I requested you to download this automatic speech recognition\n",
      "(618.52, 623.72)--> data, just this part, you go ahead and click here and download this part and move it to\n",
      "(623.72, 626.04)--> your working directory.\n",
      "(626.04, 630.88)--> For me, this is the downloaded part.parakit file.\n",
      "(630.88, 636.92)--> To extract this Parakit file into a dataset, we use load dataset.\n",
      "(636.92, 641.12)--> Then we pass in the extension and the data file, the path to the data file.\n",
      "(641.12, 642.64)--> So my data file is just here.\n",
      "(642.64, 650.6)--> So that's it. And if I run this, you\n",
      "(650.6, 658.52)--> see a traffic control asr data. If I see this is the first sample, in the first sample I\n",
      "(658.52, 666.66)--> have audio. In inside the audio I have the path to the wave file and the arrays are also there.\n",
      "(666.66, 669.64)--> Also converted into arrays, the sampling rate.\n",
      "(669.64, 673.76)--> Sampling rate is already 16,000, so I don't have to down sample or up sample them.\n",
      "(673.76, 678.56)--> Then the text, the corresponding text for which we have the audio.\n",
      "(678.56, 680.4)--> And yeah, that's it.\n",
      "(680.4, 686.96)--> To start with, let's see what I'm going to do is I'm going to import NumPy and Ipython\n",
      "(686.96, 693.4)--> audio just to play the audio.\n",
      "(693.4, 700.32)--> So I'm going to take the first sample of this air traffic control ASR data in this part\n",
      "(700.32, 707.0)--> and then I'm going to go to audio, for the first sample I'm going to audio and then array.\n",
      "(707.0, 713.0)--> So I'll get the corresponding Numpy array for this audio and also the sampling rate,\n",
      "(713.0, 738.8)--> the bottom one is sampling rate. So if you see audio original shape, so I have 117000 data points and if I play it, you can hear it by just using the iPython audio\n",
      "(738.8, 749.8)--> library and then this is the extra resampling method. If your original sampling rate is not 16,000, then you have to do it.\n",
      "(749.8, 754.68)--> But for our case, we can do it or not because it just re-samples to 1600, 16,000.\n",
      "(754.68, 759.36)--> And if I plot that, you see, this is the waveform we have.\n",
      "(759.36, 764.68)--> On the x-axis, the number of samples and the y-axis, the amplitude values.\n",
      "(764.68, 769.04)--> All right. So this is the down sampled audio we have.\n",
      "(769.04, 775.68)--> For us it did not change anything between the previous one and the later one because\n",
      "(775.68, 782.68)--> the down sampled audio and the actual original audio sample have the same frequency.\n",
      "(782.68, 792.04)--> Next we will see the targets for the whisper model. So first of all we\n",
      "(792.04, 798.36)--> take the again the first sample that we had here and now we are just taking the text because\n",
      "(798.36, 814.0)--> it is going to be the target of the whisper model, the label you can see so this is this Oscar kilo papa Mike Bravo it's pretty accurate\n",
      "(814.0, 820.0)--> the exact to what has been said in the speech and then we are going to need the whisper\n",
      "(820.0, 827.0)--> tokenizer we are going to use the whisper tokenizer dot from pre-trained. So it will\n",
      "(827.0, 834.24)--> go to the transformer repository and download this particular tokenizer. And we have that\n",
      "(834.24, 850.1)--> tokenizer now with us this tokenizer. And then what are we going to do? We are going to pass this target, this particular text to our tokenizer and we will get the\n",
      "(850.1, 852.88)--> tokenizer output.\n",
      "(852.88, 854.92)--> This tokenizer output has two parts.\n",
      "(854.92, 858.64)--> One is the input IDs and the attention mask.\n",
      "(858.64, 862.4)--> Let's see what are they?\n",
      "(862.4, 865.92)--> So first let's see what is this encoded target.\n",
      "(865.92, 870.48)--> This is nothing but what we saw before.\n",
      "(870.48, 876.32)--> So this is the initial token such as this one is start of the transcription, then the\n",
      "(876.32, 881.24)--> language, then the task and then whether timestamp is required or not.\n",
      "(881.24, 885.6)--> And then here the text exactly starts, the text as in the Oscar,\n",
      "(885.6, 893.92)--> kilo and etc. And after that towards the end, this is the end of sentence token. But it has\n",
      "(893.92, 898.72)--> repeated, you see the end of sentence token is repeated. However, we don't want end of sentence\n",
      "(898.72, 906.64)--> token, but we want minus 100 for this to fill at the end.\n",
      "(906.64, 909.6)--> And for that, we are going to use this attention mask.\n",
      "(909.6, 915.92)--> So if you see, if I plot this to the attention mask and the encoded target, you see, we have\n",
      "(915.92, 931.68)--> different values for encoded target depending upon here for this length up to this length, up to this length, but after that it is all set to the same one 50257.\n",
      "(931.68, 938.04)--> And the attention mask is only for the length we have some corresponding text and for the\n",
      "(938.04, 950.08)--> rest it is 0. What we can do is we can use this attention mask to change the values where there is 5027\n",
      "(950.08, 956.88)--> is repeated and convert that to minus 100. And for that we are going to use this masked\n",
      "(957.44, 969.72)--> fill method from PyTorch. You took the encode text dataset and in the attention mask, I just, where it\n",
      "(969.72, 976.48)--> is not equal to 1, that means where the values are 0, this regions, we are going to replace\n",
      "(976.48, 977.48)--> that with minus 1.\n",
      "(977.48, 983.72)--> So, finally, we have the encoded text which looks like this, that is all.\n",
      "(983.72, 985.28)--> And this encoded text is required\n",
      "(985.28, 986.6)--> when we calculate the loss\n",
      "(986.6, 989.84)--> because the whisper model they have trained,\n",
      "(989.84, 992.28)--> whenever it sees minus 100,\n",
      "(992.28, 995.68)--> it ignores the cross entropy loss for that part.\n",
      "(995.68, 996.52)--> That's it.\n",
      "(996.52, 999.2)--> That's why we need to do this extra modification.\n",
      "(999.2, 1001.28)--> And then we are going to use the same tokenizer\n",
      "(1001.28, 1003.24)--> to decode the encoded text.\n",
      "(1003.24, 1006.16)--> And we see we get something like this.\n",
      "(1006.16, 1008.56)--> Here we have some special tokens, the startup transcript\n",
      "(1008.56, 1010.28)--> and here.\n",
      "(1010.28, 1014.32)--> The exact test, text and the end-up text here.\n",
      "(1014.32, 1016.92)--> Next, we'll see the input feature to the model.\n",
      "(1016.92, 1018.32)--> For the input feature, we are going\n",
      "(1018.32, 1020.2)--> to use the whisper feature extractive.\n",
      "(1022.96, 1030.5)--> We initialize this feature extractor instance or the function and then we pass to the feature\n",
      "(1030.5, 1052.8)--> extractor function the raw data which is the down sampled audio, the sampling rate and or numpy array. Yeah, and whatever it extracts, we are then going to take the input feature.\n",
      "(1061.04, 1066.52)--> So yeah, and then this is what our input feature looks like. The first one is the batch size,\n",
      "(1066.52, 1070.52)--> then 80 is the number of rows\n",
      "(1070.52, 1074.12)--> and 3000 represent the time step.\n",
      "(1074.12, 1075.8)--> That means 30 second of data,\n",
      "(1075.8, 1079.0)--> it converts it into 30,000 bins.\n",
      "(1079.0, 1081.64)--> And if I just plot this using IM show,\n",
      "(1081.64, 1084.08)--> you see that it looks something like this.\n",
      "(1084.08, 1086.24)--> That only until like know like 7 or 8\n",
      "(1086.24, 1095.6)--> second we have some data and after that we don't have it. We have some activity and after that\n",
      "(1095.6, 1101.52)--> we don't have any activity this is padded with 0. This will be this input feature will be the input\n",
      "(1101.52, 1108.32)--> to the whisper model. Next we'll do the prediction using this whisper model. Next, we will do the prediction using this whisper\n",
      "(1108.32, 1112.32)--> model. For that, first we have to import the whisper model, for that we are using whisper for\n",
      "(1112.32, 1117.6)--> conditional generation and we are going to use the whisper small version of it. We imported the\n",
      "(1117.6, 1125.76)--> model and pass it to CUDA. CUDA as in because I have a GPU, I transfer this model into the GPU. So for faster generation.\n",
      "(1128.64, 1136.72)--> At the first step, let me comment them out. Yeah. Let me, what I'm going to do is model.generate.\n",
      "(1136.72, 1142.96)--> What does this do? Is it takes the input feature and gives out the\n",
      "(1156.28, 1168.0)--> basically feature and gives out basically it does this part. It takes in the log mail spectrogram and gives out the corresponding tokens.\n",
      "(1170.0, 1174.0)--> So let's do it first. And for example, let's see what is this output is.\n",
      "(1174.0, 1176.0)--> You see, this is a bunch of tokens.\n",
      "(1176.0, 1184.0)--> But now I want to convert this from token space to a text space.\n",
      "(1184.0, 1187.42)--> For that, I'm going to use tokenizer.batch decode.\n",
      "(1187.42, 1189.72)--> Then I'll pass this output into here\n",
      "(1189.72, 1191.72)--> and skip special token.\n",
      "(1191.72, 1193.52)--> If I first let me put it false.\n",
      "(1197.36, 1199.76)--> So I get the special tokens here\n",
      "(1199.76, 1202.92)--> and then the transcribed text.\n",
      "(1202.92, 1208.52)--> You can see that the transcribed text is not at all correct.\n",
      "(1208.52, 1212.2)--> It's very different from what is the target text is.\n",
      "(1212.2, 1215.64)--> The target text is this Oscar, Kilo, Papa and the Spikon.\n",
      "(1215.64, 1222.2)--> But the predicted one is RISC 100.\n",
      "(1222.2, 1225.08)--> So yeah, this is not accurate because this is new.\n",
      "(1225.08, 1226.8)--> This kind of data set is new for the model.\n",
      "(1226.8, 1229.84)--> This kind of lingo is new for the model.\n",
      "(1229.84, 1233.36)--> And we can expect that to have less accuracy.\n",
      "(1233.36, 1236.28)--> But then what I'm going to do, like how we are going\n",
      "(1236.28, 1237.96)--> to calculate the loss.\n",
      "(1237.96, 1241.72)--> And for the creator of this Whisper model\n",
      "(1241.72, 1246.08)--> has done a very good job in making our live easier.\n",
      "(1246.08, 1250.84)--> For that, instead of passing this input feature to the model.generate, we'll directly pass\n",
      "(1250.84, 1253.6)--> it to the model.\n",
      "(1253.6, 1257.24)--> We'll not only pass the input feature, but also the corresponding labels.\n",
      "(1257.24, 1260.84)--> So this encoded target is this one.\n",
      "(1260.84, 1265.42)--> If you have not, yeah, this is the encoded target by the way, the corresponding\n",
      "(1265.42, 1271.02)--> labels and wherever there is minus 100, the model will ignore the corresponding loss.\n",
      "(1271.02, 1282.28)--> All right, so let's do that. Yeah, and the output is a instance of sequence to sequence\n",
      "(1282.28, 1288.64)--> language model output. You has multiple things in there.\n",
      "(1288.64, 1293.4)--> There is the loss, there is the logits, there is past key values and everything. But we\n",
      "(1293.4, 1298.4)--> are just going to focus on the loss right now. Now I am going to print out the loss,\n",
      "(1298.4, 1309.4)--> the shape of the logit. You see the loss is 2.09 right now and the shape of the logic, the output shape is 448\n",
      "(1309.4, 1313.8)--> cross 51865.\n",
      "(1313.8, 1328.56)--> That means for, so 448 belongs to our output shape that we fixed before.\n",
      "(1328.56, 1334.0)--> And this is the logic over the entire vocabulary search.\n",
      "(1334.0, 1345.68)--> Then I'm going to, I'm going to apply a softmax function on about this axis.\n",
      "(1345.68, 1350.08)--> So I'll have a softmax axis, softmax function and then argmax.\n",
      "(1350.08, 1353.2)--> So I'll just have the 448 rows.\n",
      "(1353.2, 1371.8)--> I don't know what it is doing the practice session. And you see just by passing it to the just model function, we get a different output\n",
      "(1371.8, 1372.8)--> here.\n",
      "(1372.8, 1374.56)--> And why is that?\n",
      "(1374.56, 1380.36)--> Because when you do model.generic, we already specify the language and the task, but when\n",
      "(1380.36, 1385.52)--> you pass it through the model, you cannot mention the language or the tab.\n",
      "(1385.52, 1393.1)--> And here you see this CS token, it given it a different starting token.\n",
      "(1393.1, 1399.12)--> Also like this, it should be like startup token, then CS, then the language, then translate\n",
      "(1399.12, 1400.12)--> transcript.\n",
      "(1400.12, 1407.68)--> Here it has like randomly given multiple, you know, translate and transcribe tokens by its own because it does\n",
      "(1408.32, 1414.64)--> from nothing. It doesn't set any input token for this during the when you pass it through the model\n",
      "(1416.48, 1421.92)--> and that's one difference and that's how do a single forward pass. Once you get this logic\n",
      "(1422.64, 1425.12)--> sorry once you get this loss then you can just do\n",
      "(1425.12, 1429.52)--> loss.backward throw the model you with an optimizer to update the weights\n",
      "(1433.12, 1437.12)--> all right\n",
      "(1442.4, 1449.36)--> so before we do the fine tuning or do the one single forward and backward pass, let's see\n",
      "(1449.36, 1453.06)--> that before fine tuning on this base model, what is the performance?\n",
      "(1453.06, 1457.36)--> How does the performance looks like on first five samples?\n",
      "(1457.36, 1462.32)--> So what I'm doing is I'm just picking the target and audio signal from the dataset that\n",
      "(1462.32, 1463.52)--> I have.\n",
      "(1463.52, 1466.16)--> And here IDX represent the first sample, second sample,\n",
      "(1466.16, 1468.08)--> third sample, and so on.\n",
      "(1468.08, 1471.36)--> I take the target, the corresponding text,\n",
      "(1471.36, 1472.52)--> and the audio signal.\n",
      "(1472.52, 1475.76)--> Then I extract feature from this audio signal\n",
      "(1475.76, 1479.32)--> into input feature, and I pass it through my model.genre.\n",
      "(1479.32, 1482.2)--> I get the output, and then I convert this output\n",
      "(1482.2, 1483.96)--> into corresponding text.\n",
      "(1483.96, 1487.08)--> And then I just print it out. If I just run it,\n",
      "(1487.08, 1494.72)--> you see before training the base model is not very accurate. We are not using any metrics right now.\n",
      "(1494.72, 1500.96)--> We are just eyeballing it for now. And we see that the input, this is the true text and this is\n",
      "(1500.96, 1506.88)--> what the model predicted. Of course not accurate. Then what we are going to do is we are going to do\n",
      "(1506.88, 1509.32)--> a single forward pass through the whisper model.\n",
      "(1510.48, 1513.0)--> Right now we have the target.\n",
      "(1513.0, 1516.4)--> I'm taking the index zero, the first sample.\n",
      "(1516.4, 1519.08)--> And for that I'm getting a target and the audio signal\n",
      "(1519.08, 1520.36)--> or the input feature.\n",
      "(1520.36, 1523.4)--> I use this audio signal to generate the input feature.\n",
      "(1523.4, 1528.96)--> And also for the target because I have to do some preprocessing like this, to organize\n",
      "(1528.96, 1556.64)--> the text target into numbers, then replace the last padding numbers into minus 100. And after that, what I'm doing, I'm getting my model into train.\n",
      "(1556.64, 1560.84)--> I'm changing my model into train mode.\n",
      "(1560.84, 1565.0)--> Then I'm initializing optimizer torch with add addmw.\n",
      "(1566.2, 1568.8)--> It will optimize all the parameters of my model,\n",
      "(1568.8, 1570.8)--> both encoder and decoder.\n",
      "(1570.8, 1574.04)--> And the learning rate I'm setting is one E minus five.\n",
      "(1579.44, 1583.72)--> Then, so let me turn this one.\n",
      "(1585.04, 1586.04)--> So then what I'm doing is, yeah, So, first let me turn this on. Yes.\n",
      "(1586.04, 1593.24)--> So, then what I am doing is, yeah, I am using this single input, single input output data\n",
      "(1593.24, 1594.44)--> to fine tune my model.\n",
      "(1594.44, 1599.8)--> And here the goal is just to see whether the loss is decreasing or not.\n",
      "(1599.8, 1606.4)--> So, in the input loop, first I am taking the input feature and I already have the corresponding\n",
      "(1606.4, 1609.6)--> label which is my encoded target.\n",
      "(1609.6, 1614.64)--> When I am passing it to my model, note here I am not doing model to generate, but I am\n",
      "(1614.64, 1622.44)--> passing it to the actual model and as my model is in the GPU, I have to transfer my input\n",
      "(1622.44, 1626.88)--> feature and the labels into the GPU as well so that they can compute.\n",
      "(1626.88, 1640.64)--> I am getting the output and in the output I have the loss that I showed you before.\n",
      "(1640.64, 1648.56)--> And my goal then I am doing the loss.backward, optimizer.step, and then I'm clearing the gradient\n",
      "(1648.56, 1650.88)--> accumulations, accumulated gradients.\n",
      "(1650.88, 1652.24)--> And now I'm printing the loss.\n",
      "(1652.24, 1654.24)--> So the goal is that with iteration,\n",
      "(1654.24, 1657.2)--> I'm doing for 30 iteration, the loss should decrease.\n",
      "(1657.2, 1658.04)--> Let me run it.\n",
      "(1659.2, 1661.6)--> The starting is 3.85, and I will clearly see\n",
      "(1661.6, 1667.0)--> the loss decreasing with time.\n",
      "(1667.0, 1692.08)--> Yes, so we see it started with 3.8 and now at the performance after 10 steps.\n",
      "(1695.44, 1697.08)--> I'm taking the model into evaluation more. And then I'm just repeating the same thing\n",
      "(1697.08, 1700.04)--> that I did before this part.\n",
      "(1700.04, 1701.12)--> This part.\n",
      "(1701.12, 1703.28)--> Here we see there was a discrepancy.\n",
      "(1703.28, 1708.64)--> But now let's see how accurate it is.\n",
      "(1712.4, 1713.28)--> Yep, because we train it on the first data, it's exactly accurate.\n",
      "(1719.28, 1727.16)--> Also, the second one has also improved significantly. This one has not, this one is also not, but the goal was that how, just to see that how we can do a single forward and backward pass.\n",
      "(1727.16, 1730.08)--> And that was all about this video.\n",
      "(1730.08, 1734.16)--> Here we saw that how we can do a single forward and backward pass through the whisper model\n",
      "(1734.16, 1737.28)--> and understood other components of the whisper model.\n",
      "(1737.28, 1740.12)--> In the next video, we are going to see how we can evaluate.\n",
      "(1740.12, 1745.36)--> We need a matrix to evaluate this kind of task, the automatic speech recognition task.\n",
      "(1752.72, 1758.56)--> For that, word error rate is popularly used and we are going to see how the word error rate is calculated and we are going to see evaluate the word error rate before the training of the model\n",
      "(1758.56, 1763.92)--> and then we will check the word error rate after or during the training of the model,\n",
      "(1763.92, 1765.2)--> whether it is decreasing or not.\n"
     ]
    }
   ],
   "source": [
    "transcription = pipe(r\"C:\\Users\\User\\Desktop\\Gen-AI-Mini-Projects\\audio2text_from_mp3\\output_audio.mp3\" )\n",
    "\n",
    "\n",
    "\n",
    "formatted_lyrics = \"\"\n",
    "for line in transcription['chunks']:\n",
    "    text = line[\"text\"]\n",
    "    ts = line[\"timestamp\"]\n",
    "    formatted_lyrics += f\"{ts}-->{text}\\n\"\n",
    "\n",
    "print(formatted_lyrics.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distil WISPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "model_id = \"distil-whisper/distil-small.en\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    return_timestamps=True,\n",
    "    chunk_length_s=15,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 3.0)--> What's your interest in it, Mr. Wayne?\n",
      "(3.0, 7.0)--> I want to borrow it for uh, spill-lunking.\n",
      "(7.0, 9.0)--> Spillunking?\n",
      "(9.0, 11.0)--> Yeah, you know, cave diving.\n",
      "(12.0, 14.44)--> Expecting to run into much much gunfire on these caves.\n"
     ]
    }
   ],
   "source": [
    "transcription = pipe(r\"F:\\Gen-AI-Mini-Projects\\audio2text_from_mp3\\english_audio.mp3\" )\n",
    "\n",
    "\n",
    "\n",
    "formatted_lyrics = \"\"\n",
    "for line in transcription['chunks']:\n",
    "    text = line[\"text\"]\n",
    "    ts = line[\"timestamp\"]\n",
    "    formatted_lyrics += f\"{ts}-->{text}\\n\"\n",
    "\n",
    "print(formatted_lyrics.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
